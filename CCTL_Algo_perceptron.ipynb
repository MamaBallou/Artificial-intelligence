{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expliquer le fonctionnement de l'algorithme du Perceptron"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le perceptron est un algorithme de **classification binaire** simple pour les tâches de **classification linéaire**. Il a été inventé en 1958 par Frank Rosenblatt.\n",
    "\n",
    "Le perceptron prend en entrée un **vecteur de caractéristiques** $x$ (par exemple, la taille et le poids d'un animal) et **calcule un score de classification** $z$ en multipliant chaque caractéristique d'entrée $x_i$ par un poids correspondant $w_i$, puis en additionnant le tout :\n",
    "\n",
    "$$z = \\sum_{i=1}^{n} w_i x_i + b,$$\n",
    "\n",
    "où $n$ est le nombre de caractéristiques d'entrée, $w_i$ est le poids correspondant à la i-ème caractéristique d'entrée, $b$ est le biais (un paramètre supplémentaire qui permet au modèle de déplacer la décision) et $z$ est le score de classification.\n",
    "\n",
    "Ensuite, le score de classification est passé à une fonction d'activation qui conv**ertit le score en une sortie binaire** $y$ (0 ou 1) qui représente la classe prédite :\n",
    "\n",
    "$$y = \\begin{cases} 1, & \\text{si } z \\geq 0 \\\\ 0, & \\text{sinon} \\end{cases}$$\n",
    "\n",
    "L'**entraînement** du perceptron consiste à **ajuster les poids** $w$ et le **biais** $b$ pour minimiser l'erreur de classification sur l'ensemble de données d'entraînement. L'erreur de classification est simplement la différence entre la prédiction du modèle $y$ et la vraie classe $y_{true}$ :\n",
    "\n",
    "$$\\epsilon = y_{true} - y$$\n",
    "\n",
    "Ensuite, les poids sont ajustés selon la règle d'apprentissage du perceptron :\n",
    "\n",
    "$$w_i \\leftarrow w_i + \\eta \\epsilon x_i$$\n",
    "\n",
    "$$b \\leftarrow b + \\eta \\epsilon$$\n",
    "\n",
    "où $\\eta$ est le taux d'apprentissage, qui contrôle la taille des ajustements de poids à chaque étape de l'apprentissage.\n",
    "\n",
    "Le perceptron continue d'ajuster les poids jusqu'à ce que l'erreur de classification sur l'ensemble de données d'entraînement soit minimisée ou jusqu'à ce qu'un nombre maximal d'itérations soit atteint. Le perceptron peut également être étendu à des problèmes de classification multiclasse en utilisant des stratégies telles que l'entraînement d'un perceptron pour chaque paire de classes ou l'utilisation d'une couche de sortie softmax."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifier un cas d'usage de classification automatique par rapport à un besoin"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cf [Workshop 3](./WS3/WS_P3_-_Classification.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculer la régression logistique sur un jeu de données pour une classification binaire"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```py\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression()\n",
    "# Entrainer le modèle\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Prédire les classes sur les données de test\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# -- Évaluation du modèle --\n",
    "# Calculer la précision, le rappel et le score F1\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall: \", recall)\n",
    "print(\"F1 score: \", f1)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyser les résultats obtenus par la classification (interpréter une matrice de confusion ; AUC, courbe ROC, métriques de performance : TPR, TFR, F1-Score, précision, recall...)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Matrice de confusion** : Une matrice de confusion est un tableau qui résume la performance de classification d'un modèle. Elle compare les étiquettes de classe prédites par le modèle avec les étiquettes de classe réelles et les organise en quatre cases :\n",
    "    * True Positive (TP) : les exemples de classe positive qui ont été correctement classés.\n",
    "    * False Positive (FP) : les exemples de classe négative qui ont été incorrectement classés comme positifs.\n",
    "    * False Negative (FN) : les exemples de classe positive qui ont été incorrectement classés comme négatifs.\n",
    "    * True Negative (TN) : les exemples de classe négative qui ont été correctement classés.\n",
    "\n",
    "    La matrice de confusion permet de calculer diverses métriques de performance, notamment la précision, le rappel et le F1-score.\n",
    "<p align=\"center\">\n",
    "  <img src=\"./matrice_confusion.png\" style=\"max-width: 50%\"/>\n",
    "</p>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Courbe ROC et AUC** : La courbe ROC (Receiver Operating Characteristic) est un graphique qui représente la performance d'un modèle de classification binaire en faisant varier le seuil de décision. Elle représente le taux de vrais positifs (TPR) en fonction du taux de faux positifs (FPR) pour différents seuils de décision. L'AUC (Area Under the Curve) est une mesure de la qualité de la courbe ROC. Plus l'AUC est proche de 1, meilleure est la performance de classification du modèle.\n",
    "<p align=\"center\">\n",
    "  <img src=\"./ROC.png\" style=\"max-width: 50%\"/>\n",
    "</p>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Métriques de performance** : Les métriques de performance telles que le TPR (True Positive Rate), le TNR (True Negative Rate), le F1-score, la précision et le rappel permettent d'évaluer les performances du modèle en termes de vrais positifs, vrais négatifs, faux positifs et faux négatifs. Les métriques peuvent être calculées à partir de la matrice de confusion et permettent d'avoir une vue détaillée des performances du modèle.\n",
    "    * True Positive Rate (TPR) : le taux de vrais positifs mesure la proportion de vrais positifs parmi tous les exemples qui sont réellement positifs. C'est simplement un autre nom pour le rappel. $TPR = TP / (TP + FN)$\n",
    "    * True Negative Rate (TNR) : le taux de vrais négatifs mesure la proportion de vrais négatifs parmi tous les exemples qui sont réellement négatifs. $TNR = TN / (TN + FP)$\n",
    "    * False Positive Rate (FPR) : le taux de faux positifs mesure la proportion de faux positifs parmi tous les exemples qui sont réellement négatifs. $FPR = FP / (FP + TN)$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Améliorer le modèle de classification en fonction des résultats obtenus\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Ajouter des données** : si le modèle ne fonctionne pas bien, il est possible que les données d'entraînement ne soient pas assez représentatives du problème réel. En ajoutant plus de données, le modèle aura plus d'exemples à apprendre et pourra mieux généraliser.\n",
    "1. **Optimiser les hyperparamètres** : les hyperparamètres sont des paramètres du modèle qui ne sont pas appris à partir des données, mais qui doivent être réglés par l'utilisateur. Par exemple, le nombre de couches dans un réseau de neurones ou le seuil de décision dans un modèle de classification. Il est important d'optimiser les hyperparamètres pour obtenir de meilleurs résultats.\n",
    "1. **Utiliser un algorithme plus complexe** : si le modèle ne fonctionne pas bien, il est possible qu'il ne soit pas assez complexe pour résoudre le problème. Dans ce cas, il est possible de changer d'algorithme pour un algorithme plus complexe, comme un réseau de neurones.\n",
    "1. **Prétraiter les données** : les données brutes peuvent contenir du bruit ou des valeurs aberrantes, ce qui peut affecter la performance du modèle. Il est important de prétraiter les données en supprimant les valeurs aberrantes, en normalisant les données ou en les transformant d'une manière qui les rend plus adaptées pour le modèle.\n",
    "1. **Utiliser des techniques d'ensemble** : les techniques d'ensemble consistent à combiner plusieurs modèles pour améliorer la performance globale. Par exemple, on peut entraîner plusieurs modèles avec des données différentes et combiner leurs prédictions pour obtenir une prédiction plus robuste.\n",
    "1. **Interpréter les résultats** : en analysant les erreurs du modèle, il est possible d'identifier les cas pour lesquels le modèle se trompe le plus souvent. Cela peut aider à comprendre les limites du modèle et à identifier les domaines dans lesquels il peut être amélioré."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mesurer les résultats obtenus (identifier la métrique d'évaluation adaptée : cross-enthropie)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La **cross-entropie** : cette métrique mesure la différence entre les prédictions du modèle et les valeurs réelles. Elle est souvent utilisée comme fonction de coût pour entraîner les modèles de classification, car elle pénalise les prédictions incorrectes de manière plus forte que les prédictions correctes."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
